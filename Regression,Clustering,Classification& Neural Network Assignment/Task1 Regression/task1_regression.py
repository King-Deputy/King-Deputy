# -*- coding: utf-8 -*-
"""Task1 Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y6mKKdicQVJ6jJfmF6BGtPnv6ev0djkC

# Task 1

## Regression

### Importing Nicessary Libraries
"""

# Importing all libraries that would be needed throughout the experiment
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.linear_model import Ridge

# Importing the Houseprice data
data = pd.read_csv("/content/drive/MyDrive/Regression,clustering,ANNproject/Houseprice_data.csv")

"""### Data Inspection/ Cleaning"""

data.head()

data.info(), data.describe()

# Checking for null values
data.isnull().sum()

# After checking for duplicates, there was 5 duplicates which was then dropped
data.duplicated().sum()
data = data.drop_duplicates()

"""### Data Preprocessing"""

selected_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15', 'price']
data = data[selected_features]

# Turining specific columns to categorical  columns
data = pd.get_dummies(data, columns=['waterfront', 'view', 'condition', 'grade'], prefix=['waterfront', 'view', 'condition', 'grade'])

# Scaling the data
scaler = StandardScaler()
data[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15']] = scaler.fit_transform(data[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15']])

# Calculating the correlation matrix
correlation_matrix = data.corr()

# Plotting the correlation heatmap
plt.figure(figsize=(16, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Heatmap')
plt.show()

# Extracting the independent variables
X = data.drop('price', axis=1)

# Calculating VIF for each variable
vif_data = pd.DataFrame()
vif_data["Variable"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# Display the VIF values
print(vif_data)

from statsmodels.stats.outliers_influence import variance_inflation_factor

def calculate_vif(data_frame):
    # Calculate VIF for each variable
    vif_data = pd.DataFrame()
    vif_data["Variable"] = data_frame.columns
    vif_data["VIF"] = [variance_inflation_factor(data_frame.values, i) for i in range(data_frame.shape[1])]
    return vif_data

# Your original dataframe
X = data.drop('price', axis=1)

# Loop to iteratively drop variables with high VIF
while True:
    vif_data = calculate_vif(X)
    max_vif = vif_data['VIF'].max()

    if max_vif > 5:
        # Drop the variable with the highest VIF
        variable_to_drop = vif_data[vif_data['VIF'] == max_vif]['Variable'].values[0]
        X = X.drop(variable_to_drop, axis=1)
    else:
        break

# Displaying the final dataframe with reduced multicollinearity
print(X)

"""**This are the Columns that was dropped** sqft_living, sqft_above, sqft_basement, waterfront_0, waterfront_1, view_0, view_1, view_2, view_3, view_4, condition_1, condition_2, condition_3, condition_4, condition_5, grade_1, grade_3, grade_4, grade_5, grade_6, grade_7, grade_8, grade_9, grade_10, grade_11, grade_12, grade_13

**These features were highly correlated with other features in the dataset, making them redundant and causing issues like infinite VIF values. Dropping them helps to address multicollinearity.**
"""

# Slitting to test and train data
y = data['price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""### Training the Model"""

# Using Linear Regression Model
model = LinearRegression()
model.fit(X_train, y_train)

# Making predictions on the test set
y_pred = model.predict(X_test)

# Plotting residuals against one input (bedrooms)
plt.figure(figsize=(10, 6))
plt.scatter(X_test['bedrooms'], y_test - y_pred, c='blue', marker='o', label='Residuals')
plt.axhline(y=0, color='red', linestyle='--', linewidth=2, label='Zero Residual Line')
plt.xlabel('bedrooms')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.legend()
plt.show()

# Measuring the effectiveness of the model by using Mse and R2 score
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean Squared Error: ", mse)
print("R-squared (R2) Score: ", r2)

"""### Visualising the predicted model"""

# Creating a scatter plot of actual vs. predicted prices and showing the regression line
plt.scatter(y_test, y_pred, color='blue', label='Data Points')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', linewidth=2, label='Regression Line')

plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices")
plt.title("Actual Prices vs. Predicted Prices")
plt.legend()

plt.show()

"""### Improving the Linear Regression algorithm

#### Using GridSearchCv to get the best parameters
"""

# Defining the parameter grid to search over
param_grid = {
    'fit_intercept': [True, False],
    'positive': [True, False],
    'alpha': [0.1, 0.5, 1.0, 5.0]  #regularization strength values
}

# Creating a GridSearchCV object for Ridge regression
ridge_grid_search = GridSearchCV(
    estimator=Ridge(),
    param_grid=param_grid,
    scoring='neg_mean_squared_error',
    cv=5
)

# Fitting the GridSearchCV object to your data
ridge_grid_search.fit(X_train, y_train)

# Getting the best parameters and the best estimator
best_params_ridge = ridge_grid_search.best_params_
best_ridge_model = ridge_grid_search.best_estimator_

# Using the best estimator for predictions
y_pred2 = best_ridge_model.predict(X_test)

# Measuring the effectiveness of the Improved model by using Mse and R2 score
mse = mean_squared_error(y_test, y_pred2)
r2 = r2_score(y_test, y_pred2)

print("Best Parameters: ", best_params_ridge)
print("Mean Squared Error (Best Model): ", mse)
print("R-squared (R2) Score (Best Model): ", r2)

# Creating a scatter plot of actual vs. predicted prices for the best parameter model and showing the regression line
plt.scatter(y_test, y_pred2, color='blue', label='Data Points')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', linewidth=2, label='Regression Line')

plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices (Best Model)")
plt.title("Actual Prices vs. Predicted Prices (Best Model)")
plt.legend()

plt.show()

"""### Exploring Advanced Regression models

#### Using RandomForest Regression
"""

# Creating a Random Forest Regressor
rf_regressor = RandomForestRegressor(random_state=42)

rf_regressor.fit(X_train, y_train)
y_pred_rf = rf_regressor.predict(X_test)

# E# Measuring the effectiveness of the Random Forest model by using Mse and R2 score
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print("Random Forest Regression - Mean Squared Error: ", mse_rf)
print("Random Forest Regression - R-squared (R2) Score: ", r2_rf)

# Create a scatter plot of actual vs. predicted prices and showing the regression line
plt.scatter(y_test, y_pred_rf, color='blue', label='Data Points')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', linewidth=2, label='Regression Line')

plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices (Random Forest)")
plt.title("Actual Prices vs. Predicted Prices (Random Forest)")
plt.legend()

plt.show()

